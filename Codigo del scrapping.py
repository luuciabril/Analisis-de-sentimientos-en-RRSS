# -*- coding: utf-8 -*-
"""Notebook_Instagram_Comments_Sentiments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MB31-E2Yz4yJGfUdzsq1iaQYGs6olK1J

#Cuaderno para analizar sentimientos de comentarios de Instagram
"""

# Instalación de dependencias necesarias
!pip install selenium beautifulsoup4 config chromedriver-autoinstaller
# Actualizar y instalar Chromium y ChromeDriver
!apt-get update
!apt-get install -y chromium-browser chromium-chromedriver wget
# Descargar Google Chrome y ChromeDriver si es necesario
!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!dpkg -i google-chrome-stable_current_amd64.deb
!apt-get -f install -y

# Instalar automáticamente la versión correcta de ChromeDriver
import chromedriver_autoinstaller
# Instalar automáticamente la versión correcta de ChromeDriver
chromedriver_autoinstaller.install()

import config
import math
import sys
import time
import requests
import re
import json
import os
import csv
import pandas as pd
import requests
import datetime
sys.path.append('/usr/lib/chromium-browser/chromedriver')
from selenium import webdriver
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from IPython.display import Image, display
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException

# Configuración del navegador
chrome_options = Options()
chrome_options.add_argument("--headless")  # Sin interfaz gráfica
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# Inicializar el navegador
driver = webdriver.Chrome(options=chrome_options)

# Probar conexión
driver.get("https://www.instagram.com/")

# Aceptar cookies
try:
    allow_cookies_button = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "button.allow-cookies"))
    )
    allow_cookies_button.click()
    print("Cookies accepted")
except Exception as e:
    print("Cookies button not found or already accepted", e)

#Iniciar sesión
try:
    # Target username y password
    username = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "input[name='username']"))
    )
    password = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "input[name='password']"))
    )

    # Enter username y password
    username.clear()
    username.send_keys("luciabril12")  # Sustituye con 'config.username' si usas un módulo config
    password.clear()
    password.send_keys("luciabril1212")  # Sustituye con 'config.password' si usas un módulo config

    # Target the login button and click it
    button = WebDriverWait(driver, 2).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, "button[type='submit']"))
    )
    button.click()

    print("Inicio de sesión realizado con éxito.")

except Exception as e:
    print(f"Error durante el proceso de inicio de sesión: {e}")

# Tomar una captura de pantalla
screenshot_path = "/content/screenshot.png"
driver.save_screenshot(screenshot_path)

# Visualizar la captura de pantalla en Colab
display(Image(filename=screenshot_path))

#Verificacion con codigo al correo

# Variable con el valor a ingresar
email_value = "415747"

try:
    # Esperar a que el elemento esté visible y accesible
    email_input = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, ":r7:"))
    )

    # Limpiar el campo (por si ya contiene texto) y escribir el valor
    email_input.clear()
    email_input.send_keys(email_value)

    # Confirmar el ingreso con ENTER (opcional, si la interacción lo requiere)
    # email_input.send_keys(Keys.RETURN)

    print("El codigo se ingresó correctamente.")
except TimeoutException:
    print("El campo de codigo no se encontró o tardó demasiado en cargar.")


try:
    # Esperar que el botón "Continuar" esté presente y sea clickeable
    continuar_button = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.XPATH, '//span[text()="Continue"]'))
    )

    # Hacer clic en el botón
    continuar_button.click()

    print("Botón 'Continuar' clickeado correctamente.")
except TimeoutException:
    print("El botón 'Continuar' no se encontró o tardó demasiado en cargar.")

#Guardar informacion de inicio de sesion
# Esperar a que el botón "Not now" sea clicable
not_button = WebDriverWait(driver, 10).until(
    EC.element_to_be_clickable((By.XPATH, '//div[contains(@role, "button") and text()="Not now"]'))
)

# Hacer clic en el botón
not_button.click()

#click boton de busqueda
try:
    # Locate and click on the search button
    search_button = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.CSS_SELECTOR, 'svg[aria-label="Search"]'))
    )
    search_button.click()
except TimeoutException:
    print("The search button was not found or is not clickable.")
    driver.save_screenshot("search_debug.png")

#Busqueda de cuenta
try:
    # Espera hasta que el campo de búsqueda sea clicable
    searchbox = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.XPATH, "//input[@placeholder='Search']"))
    )
    searchbox.clear()

    # Busca el @handle o palabra clave
    keyword = "@sumar_oficial"
    searchbox.send_keys(keyword)
except TimeoutException:
    print("No se pudo encontrar el campo de búsqueda.")
    driver.save_screenshot("searchbox_debug.png")

#Ingreso a la cuenta
try:
    # Verifica si la palabra clave comienza con "@"
    if keyword.startswith("@"):
        # Elimina el símbolo "@"
        keyword = keyword[1:]

    print(f"Buscando el resultado para: {keyword}")  # Imprime la palabra clave para verificar que esté correcta

    # Espera hasta que el primer resultado sea visible
    first_result = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.XPATH, f'//span[text()="{keyword}"]'))
    )

    # Haz clic en el primer resultado
    first_result.click()

except TimeoutException:
    print("Tiempo de espera agotado, no se encontró el campo de búsqueda.")
    driver.save_screenshot("searchbox_debug.png")
except NoSuchElementException:
    print(f"No se pudo encontrar el elemento con el texto: {keyword}")

#deeeeeppppssssscrolllllllll
# Get the initial page height
initial_height = driver.execute_script("return document.body.scrollHeight")

# Create a list to store htmls
soups = []

while True:
    # Scroll down to the bottom of the page
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

    # Wait for a moment to allow new content to load (adjust as needed)
    time.sleep(3)

    # Parse the HTML
    html = driver.page_source

    # Create a BeautifulSoup object from the scraped HTML
    soups.append(BeautifulSoup(html, 'html.parser'))

    # Get the current page height
    current_height = driver.execute_script("return document.body.scrollHeight")

    if current_height == initial_height:
        break  # Exit the loop when you can't scroll further

    initial_height = current_height  # Update the initial height for the next iteration

# Lista para almacenar las URLs
post_urls = []

for soup in soups:
    # Encontrar todos los elementos <a> con atributo href
    anchors = soup.find_all('a', href=True)

    # Recorrer cada enlace y guardar los hrefs
    for anchor in anchors:
        href = anchor['href']
        print(href)  # Si deseas seguir viendo los hrefs
        post_urls.append(href)  # Agregar la URL a la lista

# Filtrar solo las URLs que contienen "/p/"
filtered_urls = [url for url in post_urls if "/p/" in url]

# Imprimir las URLs filtradas
print("Filtered URLs:")
for url in filtered_urls:
    print(url)

# Lista para almacenar los datos JSON
json_list = []

# Define los parámetros de consulta a añadir
query_parameters = "__a=1&__d=dis"

# Iterar sobre todas las URLs únicas
for url in filtered_urls:
    try:
        # Modificar la URL agregando los parámetros de consulta
        modified_url = f"https://www.instagram.com{url}?{query_parameters}"

        # Obtener la página con la URL modificada
        driver.get(modified_url)

        # Esperar a que el contenido cargue (ajustar el tiempo si es necesario)
        time.sleep(1)

        # Esperar a que el <pre> con los datos JSON esté presente
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//pre'))
        )

        # Encontrar el <pre> que contiene los datos JSON
        pre_tag = driver.find_element(By.XPATH, '//pre')

        # Extraer los datos JSON del contenido del <pre>
        json_script = pre_tag.text

        # Parsear el JSON
        json_parsed = json.loads(json_script)

        # Añadir el JSON parseado a la lista
        json_list.append(json_parsed)
    except (NoSuchElementException, TimeoutException, json.JSONDecodeError) as e:
        # Manejo de errores si no se puede procesar la URL
        print(f"Error procesando la URL {url}: {e}")

# Al final, json_list contendrá los datos extraídos de las páginas
print(f"Total de elementos extraídos: {len(json_list)}")

# Lists to store URLs and corresponding dates
all_urls = []
all_dates = []

# Iterate through each JSON data in the list
for json_data in json_list:

    # Extract the list from the 'items' key
    item_list = json_data.get('items', [])

    # Iterate through each item in the 'items' list
    for item in item_list:

        # Extract the date the item was taken
        date_taken = item.get('taken_at')  # Move this line inside the loop

        # Check if 'carousel_media' is present
        carousel_media = item.get('carousel_media', [])

        # Iterate through each media in the 'carousel_media' list
        for media in carousel_media:

            # Extract the image URL from the media
            image_url = media.get('image_versions2', {}).get('candidates', [{}])[0].get('url')

            if image_url:
                # Add the image URL and corresponding date to the lists
                all_urls.append(image_url)
                all_dates.append(date_taken)
                print(f"carousel image added")

            # Extract the video URL from the media
            video_versions = media.get('video_versions', [])
            if video_versions:
                video_url = video_versions[0].get('url')
                if video_url:

                    # Add the video URL and corresponding date to the lists
                    all_urls.append(video_url)
                    all_dates.append(date_taken)
                    print(f"carousel video added")

        # Handle cases of unique image, instead of carousel
        image_url = item.get('image_versions2', {}).get('candidates', [{}])[0].get('url')
        if image_url:

            # Add the image URL and corresponding date to the lists
            all_urls.append(image_url)
            all_dates.append(date_taken)
            print(f"single image added")

        # Check if 'video_versions' key exists
        video_versions = item.get('video_versions', [])
        if video_versions:
            video_url = video_versions[0].get('url')
            if video_url:
                all_urls.append(video_url)
                all_dates.append(date_taken)
                print(f"video added")

# Print or use all collected URLs as needed
print(len(all_urls))

"""### Obtener los metadatos"""

# Lists to store extracted data
data = []

# Iterate through each JSON data in the list
for json_data in json_list:

    # Extract the list from the 'items' key
    item_list = json_data.get('items', [])

    # Iterate through each item in the 'items' list
    for item in item_list:
        # General fields
        date_taken = item.get('taken_at')
        post_id = item.get('pk')
        post_code = item.get('code')
        media_type = item.get('media_type')
        like_count = item.get('like_count')
        comment_count = item.get('comment_count')

        # User information
        user_info = item.get('user', {})
        username = user_info.get('username')
        full_name = user_info.get('full_name')
        is_private = user_info.get('is_private')
        is_verified = user_info.get('is_verified')
        hd_profile_pic_url = user_info.get('hd_profile_pic_url_info', {}).get('url')

        # Caption
        caption = item.get('caption')  # Puede ser None
        caption_text = caption.get('text') if caption else None

        # Location
        location_info = item.get('location', {})
        location_name = location_info.get('name')
        location_lat = location_info.get('lat')
        location_lng = location_info.get('lng')

        # Media URLs
        media_urls = []

        # Handle carousel media
        carousel_media = item.get('carousel_media', [])
        for media in carousel_media:
            # Image URLs
            image_url = media.get('image_versions2', {}).get('candidates', [{}])[0].get('url')
            if image_url:
                media_urls.append(image_url)

            # Video URLs
            video_versions = media.get('video_versions', [])
            if video_versions:
                video_url = video_versions[0].get('url')
                if video_url:
                    media_urls.append(video_url)

        # Handle single media (not carousel)
        if not carousel_media:
            # Image URLs
            image_url = item.get('image_versions2', {}).get('candidates', [{}])[0].get('url')
            if image_url:
                media_urls.append(image_url)

            # Video URLs
            video_versions = item.get('video_versions', [])
            if video_versions:
                video_url = video_versions[0].get('url')
                if video_url:
                    media_urls.append(video_url)

        # Add extracted data to the list
        data.append({
            "date_taken": date_taken,
            "post_id": post_id,
            "post_code": post_code,
            "media_type": media_type,
            "like_count": like_count,
            "comment_count": comment_count,
            "username": username,
            "full_name": full_name,
            "is_private": is_private,
            "is_verified": is_verified,
            "hd_profile_pic_url": hd_profile_pic_url,
            "caption_text": caption_text,
            "location_name": location_name,
            "location_lat": location_lat,
            "location_lng": location_lng,
            "media_urls": media_urls
        })

profile_stats = pd.DataFrame(data)
profile_stats['date_taken'] = pd.to_datetime(profile_stats['date_taken'], unit='s', utc=True)
profile_stats

#Guardar el texto de los mensajes en un fichero para analizar
profile_stats['caption_text'].replace(to_replace=[r"\\t|\\n|\\r", "\t|\n|\r"], value=["",""], regex=True, inplace=True)
profile_stats['caption_text'].to_csv("caption_text.txt", index=False, header=False)
#

"""### Analizar los sentimientos"""

#Cargar el diccionario que quérramos usar
from google.colab import files
files.upload()

#CREO DICCIONARIO
filenameAFINN = 'sentistrength_es.txt'
afinn = pd.read_csv(filenameAFINN,
                          header=None, # no column names
                          sep='\t',  # tab sepeated
                          names=['term', 'value']) #new column names
afinn = afinn.set_index('term')
afinn = afinn['value'].to_dict()

pattern_split = re.compile(r"\W+")
def sentiment(text):
    """
    Returns a float for sentiment strength based on the input text.
    Positive values are positive valence, negative value are negative valence.
    """
    words = pattern_split.split(text.lower())
    sentiments = [afinn.get(word, 0) for word in words]
    if sentiments:
        # How should you weight the individual word sentiments?
        # You could do N, sqrt(N) or 1 for example. Here I use sqrt(N)
        sentiment = float(sum(sentiments))/math.sqrt(len(sentiments))

    else:
        sentiment = 0
    return sentiment

#En caso de modificar el diccionario:
#afinn.update({'dolor': -5, 'tormenta': -5})

#ANALIZO
original_stdout = sys.stdout

with open('sentiments.txt', 'w') as f:
    sys.stdout = f
    #Read islam.txt from the same directory of the Notebook
    f=open("caption_text.txt", "r", encoding="utf8", errors='ignore')
    f1=f.readlines()
    for x in f1:
        score = sentiment(x)
        print(x.rstrip('\n \n'), "\t", score)
    sys.stdout = original_stdout

!wc sentiments.txt

results = pd.read_csv('sentiments.txt', sep='\t', header=None, names=['message', 'score'])
results

#Se añade a mi DF
profile_stats['sentiments'] = results['score']
profile_stats

profile_stats['sentiments'].mean()

profile_stats['sentiments'].min()

profile_stats['sentiments'].max()

#guardamos archivo para descarga
profile_stats.to_csv("profile_stats.csv", index=False, sep='\t', escapechar='\\')

from google.colab import files
files.download('profile_stats.csv')

driver.close()